{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer perceptron\n",
    "class SLP(object):    \n",
    "    def __init__(self, n_weights = 3, rate=1):\n",
    "        \n",
    "        # Neural network variables\n",
    "        self.n_weights = n_weights # number of weights\n",
    "        self.rate = rate # learning rate \n",
    "        \n",
    "        # Set initial random weights for the network\n",
    "        W = np.array([random.uniform(-1,1) for _ in range(self.n_weights)])\n",
    "        #W = np.zeros(self.n_weights)\n",
    "        W = np.insert(W, 0, 1, axis=0) # inserting bias\n",
    "        self.W = W.reshape((-1,1))  \n",
    "                \n",
    "    \n",
    "    # Activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Activation function derivative\n",
    "    def d_sigmoid(self, x): # derivate of sigmoid\n",
    "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
    "    \n",
    "    def V(self, X):\n",
    "        return np.diag(self.d_sigmoid(X @ self.W).flatten())\n",
    "    \n",
    "    def H(self, X):\n",
    "        #print(f'V: {self.V(X)}')\n",
    "        return X.T @ self.V(X) @ X\n",
    "\n",
    "    \n",
    "    # Binary Cross-Entropy loss function\n",
    "    def loss(self, Z, Y):\n",
    "        return np.asscalar((-1/len(Y)) * ( Y.T @ np.log(Z + (1.e-10)) + (1 - Y).T @ np.log(1 - Z + (1.e-10)) )) \n",
    "\n",
    "    # Gradient of loss function\n",
    "    def gradient_loss(self, Z, Y):\n",
    "        dL = Y.T @ (1/Z) + (1 - Y).T @ (1/(Z - 1))\n",
    "        return np.asscalar((-1/len(Y)) * dL)\n",
    "    \n",
    "    # Foward step\n",
    "    def forward(self, X):\n",
    "        V =  X @ self.W\n",
    "        \n",
    "        Z = self.sigmoid(V)\n",
    "        \n",
    "        return V,Z\n",
    "    \n",
    "    # Train neural network\n",
    "    def train(self, X, Y):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1) # inserting bias\n",
    "        k = 0\n",
    "        print(\"Epoch {}:\".format(k))\n",
    "        V, Z = self.forward(X)\n",
    "        print(\"loss: {}\".format(self.loss(Z,Y)))\n",
    "        print(\"Predict: {}\".format(self.predict(X,Y,bias=False)))\n",
    "        loss = np.inf        \n",
    "        # Perform a gradient descent algorithm\n",
    "        while abs(loss) > 0.1 and k < 1000:\n",
    "            k = k + 1\n",
    "            #W_new = self.W - self.rate * self.gradient_loss(Z, Y) * X.T @ self.d_sigmoid(V)\n",
    "            W_new = self.W + self.rate * np.linalg.pinv(self.H(X)) @ X.T @ (Y - Z)\n",
    "\n",
    "            self.W = W_new \n",
    "\n",
    "            print(\"Epoch {}:\".format(k))\n",
    "\n",
    "            V, Z = self.forward(X)\n",
    "            loss = self.loss(Z,Y)\n",
    "            print(f\"loss: {loss}\")\n",
    "            print(f\"Predict: {self.predict(X,Y,bias=False)}\")\n",
    "    \n",
    "    # Predict class of a sample\n",
    "    def predict(self, X, Y, bias=True):\n",
    "        if bias:\n",
    "            X = np.insert(X,0,1,axis=1) # inserting bias\n",
    "        Z = self.forward(X)[1].flatten().round()\n",
    "        errors = 0\n",
    "        for i,pred in enumerate(Z):\n",
    "            if pred != Y[i]:\n",
    "                errors += 1\n",
    "\n",
    "        acc = 1 - (errors / len(Y))\n",
    "        return acc       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SLP(n_weights=30, rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "y = data.target.reshape(-1,1)\n",
    "\n",
    "X = data.data\n",
    "\n",
    "\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss: 3.0893689668752793\n",
      "Predict: 0.18461538461538463\n",
      "Epoch 1:\n",
      "loss: 0.7151972266169087\n",
      "Predict: 0.5824175824175823\n",
      "Epoch 2:\n",
      "loss: 0.5903976164593269\n",
      "Predict: 0.6461538461538461\n",
      "Epoch 3:\n",
      "loss: 0.5074447470903242\n",
      "Predict: 0.7208791208791209\n",
      "Epoch 4:\n",
      "loss: 0.44576768436733305\n",
      "Predict: 0.7912087912087912\n",
      "Epoch 5:\n",
      "loss: 0.39711228150307726\n",
      "Predict: 0.8461538461538461\n",
      "Epoch 6:\n",
      "loss: 0.35728757382767873\n",
      "Predict: 0.8901098901098901\n",
      "Epoch 7:\n",
      "loss: 0.3238556902796361\n",
      "Predict: 0.9186813186813186\n",
      "Epoch 8:\n",
      "loss: 0.29526484565489075\n",
      "Predict: 0.9296703296703297\n",
      "Epoch 9:\n",
      "loss: 0.27046246728036977\n",
      "Predict: 0.9406593406593406\n",
      "Epoch 10:\n",
      "loss: 0.2487005596350731\n",
      "Predict: 0.9494505494505494\n",
      "Epoch 11:\n",
      "loss: 0.22942866281511193\n",
      "Predict: 0.9516483516483516\n",
      "Epoch 12:\n",
      "loss: 0.21223080364912533\n",
      "Predict: 0.9538461538461538\n",
      "Epoch 13:\n",
      "loss: 0.1967862597733566\n",
      "Predict: 0.9582417582417583\n",
      "Epoch 14:\n",
      "loss: 0.18284388676063254\n",
      "Predict: 0.9626373626373627\n",
      "Epoch 15:\n",
      "loss: 0.1702043464834851\n",
      "Predict: 0.9648351648351648\n",
      "Epoch 16:\n",
      "loss: 0.1587070391601393\n",
      "Predict: 0.967032967032967\n",
      "Epoch 17:\n",
      "loss: 0.14822015206950886\n",
      "Predict: 0.9692307692307692\n",
      "Epoch 18:\n",
      "loss: 0.13863320528455733\n",
      "Predict: 0.9714285714285714\n",
      "Epoch 19:\n",
      "loss: 0.12985175033043023\n",
      "Predict: 0.9758241758241758\n",
      "Epoch 20:\n",
      "loss: 0.12179376837022575\n",
      "Predict: 0.9758241758241758\n",
      "Epoch 21:\n",
      "loss: 0.11438724868850385\n",
      "Predict: 0.9758241758241758\n",
      "Epoch 22:\n",
      "loss: 0.10756852432948004\n",
      "Predict: 0.9758241758241758\n",
      "Epoch 23:\n",
      "loss: 0.10128108834586452\n",
      "Predict: 0.9802197802197802\n",
      "Epoch 24:\n",
      "loss: 0.09547472023383816\n",
      "Predict: 0.9824175824175824\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nn.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526316"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
