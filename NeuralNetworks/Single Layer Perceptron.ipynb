{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer perceptron\n",
    "class SLP(object):    \n",
    "    def __init__(self, n_neurons=3, rate=1):\n",
    "        \n",
    "        # Network Architecture Variables\n",
    "        self.n_neurons = n_neurons\n",
    "        self.rate = rate # learning rate        \n",
    "        # Set weights for the network\n",
    "        W = np.array(np.random.randn(self.n_neurons))\n",
    "        W = np.insert(W, 0, 1, axis=0) # inserting bias\n",
    "        self.W = W.reshape((n_neurons + 1,1))       \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def d_sigmoid(self, x): # derivate of sigmoid\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    \n",
    "    def loss(self, Z, Y):\n",
    "        Z = Z.flatten() # reshape matrix to column vector\n",
    "        return np.asscalar((-1/len(Z)) * (np.dot(Y, np.log(Z + (1.e-10))) + np.dot((1 - Y), np.log(1 - Z + (1.e-10))))) \n",
    "    \n",
    "    def gradient_loss(self, Z, Y):\n",
    "        Z = Z.flatten() # reshape matrix to column vector\n",
    "        dL = np.dot(Y, 1/Z) + np.dot((1 - Y), -1/(1 - Z))\n",
    "        return np.asscalar((-1/len(Z)) * dL)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        V = X @ self.W \n",
    "        Z = self.sigmoid(V)\n",
    "        print(\"Z {}:\".format(Z))\n",
    "\n",
    "#         print(\"Z: {}\".format(Z))\n",
    "        return V,Z\n",
    "    \n",
    "    # CLASSIFICATION PREDICTION\n",
    "    def train(self, X, Y):\n",
    "        n = 0\n",
    "        print(\"Epoch {}:\".format(n))\n",
    "        V, Z = self.forward(X)\n",
    "        print(\"loss: {}\".format(self.loss(Z,Y)))\n",
    "        while self.loss(Z, Y) > 0.1 and n < 10000:\n",
    "            n = n + 1\n",
    "            W_new = self.W - self.rate * self.gradient_loss(Z, Y) * X.transpose() @ self.d_sigmoid(V) \n",
    "            self.W = W_new\n",
    "            print(self.W.shape)\n",
    "\n",
    "            print(\"Epoch {}:\".format(n))\n",
    "            V, Z = self.forward(X)\n",
    "            print(\"loss: {}\".format(self.loss(Z,Y)))\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = self.forward(x)[1]\n",
    "        return 1 if z >= 0.5 else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SLP(n_neurons=3, rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train gender classifier\n",
    "X = [[181, 80, 44], [177, 70, 43], [160, 60, 38], [154, 54, 37], \n",
    "[166, 65, 40], [190, 90, 47], [175, 64, 39], [177, 70, 40], [159, 55, 37],\n",
    "[171, 75, 42], [181, 85, 43]]\n",
    "X = np.insert(X,0,1,axis=1) # inserting bias\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "Y = np.array([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1])\n",
    "\n",
    "nn.train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 155, 49, 37])\n",
    "\n",
    "print(nn.predict(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
